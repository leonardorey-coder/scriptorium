El origen y evolución del concepto de calidad en el desarrollo del software.
La preocupación por la calidad tiene raíces anteriores a la informática, proveniente de la manufactura y el control estadístico. Pioneros como W. E. Deming y J. M. Juran establecieron principios fundamentales (ciclo PDCA de mejora continua, los “14 puntos” de Deming y la “trilogía de la calidad” de Juran) aplicables también al software (Deming, 1986; Juran, 1951) . En particular, Juran definió la calidad como “fitness for use” («aptitud para el uso»), concepto que luego se tradujo al entorno del software (Juran, 1951) 
En las décadas de 1950–60 el software era incipiente y la calidad del software recaía casi exclusivamente en el programador. Sin embargo, al aumentar la complejidad y el uso de sistemas críticos surgió la llamada «crisis del software» (1968), que evidenció proyectos retrasados, sobrecostos y productos poco confiables. Como reacción, a finales de los 60 EE. UU. creó la noción de ingeniería de software, y en los 70 organismos como la NASA y el Departamento de Defensa (DoD) exigieron prácticas formales de calidad. Por ejemplo, en la década de 1970 el DoD integró requisitos de aseguramiento de calidad en los contratos de software (p.ej. normas militares y planes de SQA). Estas iniciativas sentaron las bases del Software Quality Assurance, incorporando pruebas sistemáticas, revisiones y métricas desde etapas tempranas del desarrollo.
Aportes de pioneros en calidad (Deming, Juran, McCall, Boehm)
Durante las décadas de 1970 y 1980, comenzaron a desarrollarse modelos teóricos para evaluar y mejorar la calidad del software. Uno de los primeros fue el modelo de calidad de McCall (1977), que propuso una clasificación de atributos de calidad en tres dimensiones: operación del producto, capacidad de cambio y adaptación. Poco después, Barry Boehm (1978) introdujo un modelo jerárquico más detallado que incluía factores como la usabilidad, eficiencia, testabilidad y portabilidad. Estos modelos representaron un cambio de paradigma al permitir medir la calidad no solo en términos técnicos, sino también desde la experiencia del usuario.
Paralelamente, los conceptos de calidad en la industria manufacturera, liderados por expertos como W. Edwards Deming y Joseph Juran, empezaron a ser adaptados al software. Deming promovió la mejora continua a través del ciclo PDCA (Plan-Do-Check-Act), mientras que Juran introdujo la idea de “calidad como adecuación al uso”. Aunque sus ideas nacieron en el contexto industrial, influyeron notablemente en la forma en que se concibió el aseguramiento de la calidad en sistemas informáticos.
Modelos y normas de calidad: ISO e IEEE
En los años 90, surgieron estándares internacionales que ayudaron a formalizar los procesos de calidad. La norma ISO/IEC 9126 (1991) definió un conjunto de características para evaluar la calidad del software como producto, incluyendo funcionalidad, confiabilidad, usabilidad, eficiencia, mantenibilidad y portabilidad. Esta norma fue sucedida por la ISO/IEC 25010 (2011), que actualizó y amplió el modelo con nuevas dimensiones como la compatibilidad y la seguridad, adaptándose a las exigencias del software moderno.
Al mismo tiempo, el IEEE desarrolló estándares técnicos para definir buenas prácticas en el desarrollo de software, como el IEEE 730 (para el aseguramiento de calidad) y el IEEE 829 (para la documentación de pruebas). Además, el Capability Maturity Model (CMM) y posteriormente el CMMI, promovidos por el Software Engineering Institute, proporcionaron un marco de referencia para evaluar la madurez de los procesos de desarrollo y gestión de calidad en organizaciones.
En la actualidad, aunque las metodologías ágiles han cambiado el enfoque tradicional del desarrollo, la calidad sigue siendo un eje fundamental. Se ha evolucionado hacia prácticas más iterativas y colaborativas, pero sin dejar de lado los principios fundamentales heredados de modelos y normas anteriores.Los métodos analizados incluyen:  
 
Historia de la Calidad del Software: Hitos Cronológicos
Desde los inicios de la programación, la preocupación por la calidad del software ha sido un tema central, especialmente a medida que los sistemas informáticos comenzaron a tener un impacto directo en la industria, la seguridad y la vida cotidiana. A continuación, se presentan los hitos más importantes en la evolución de la calidad del software de forma cronológica.
Años 1950-1960: Los inicios y la programación artesanal
En la década de 1950, el software se desarrollaba principalmente de forma manual y específica para el hardware. No existían estándares formales de calidad, y los errores (bugs) eran comunes. Fue en 1947 cuando Grace Hopper popularizó el término “bug” al encontrar una polilla en un relé. A pesar de ello, la calidad era percibida más como una responsabilidad del programador individual que como una práctica sistemática.
A finales de los años 60, se acuñó el término “crisis del software” para describir los frecuentes fallos en los sistemas informáticos, el aumento de los costos de mantenimiento y la dificultad para cumplir con los tiempos de entrega. Esta crisis despertó una preocupación profunda por establecer prácticas más rigurosas y predecibles en el desarrollo de software.
Década de 1970: Nace la ingeniería del software
La creciente complejidad de los sistemas llevó a la creación formal de la disciplina de ingeniería del software, destacando la importancia de métodos estructurados y planificación. En 1972, Barry Boehm propuso el modelo de calidad basado en características como fiabilidad, eficiencia y mantenibilidad, sentando las bases de la medición de la calidad. Asimismo, en 1979, Tom DeMarco introduce la ingeniería de requerimientos, mejorando la especificación de lo que debe hacer un software desde etapas tempranas. Se reconoció entonces que la calidad no solo se construía al final con pruebas, sino que debía integrarse desde el inicio del ciclo de vida.
Años 1980: Modelos de calidad y estandarización
Durante esta década se comenzaron a aplicar estándares formales y modelos de mejora continua. En 1987, la Organización Internacional de Normalización (ISO) publicó la norma ISO 9001, que, aunque inicialmente no era específica para software, pronto fue adoptada por empresas tecnológicas como un marco de gestión de calidad. En paralelo, el modelo CMM (Capability Maturity Model), creado por el SEI (Software Engineering Institute), ayudó a las organizaciones a medir y mejorar su madurez en procesos de desarrollo de software, clasificándolas en cinco niveles que iban desde procesos caóticos hasta procesos optimizados.
Este período también vio el auge de las métricas de software, como el análisis de complejidad ciclomática, la cobertura de pruebas y las métricas de defectos, lo que permitó una evaluación más objetiva de la calidad.
Años 1990: Enfoque en procesos y satisfacción del cliente
En esta etapa, la calidad comenzó a considerarse desde una perspectiva más amplia: no solo se trataba de productos sin errores, sino de que cumplieran con las necesidades del cliente. Aparecen modelos como SPICE/ISO 15504 y metodologías como Six Sigma, que ayudan a controlar y mejorar los procesos. La verificación y validación continua se convirtieron en prácticas estándar, con énfasis en asegurar que el producto final reflejara correctamente los requerimientos iniciales.
También durante esta década, el enfoque en la documentación exhaustiva y las auditorías de calidad ayudó a consolidar buenas prácticas en entornos regulados, como la industria aeronáutica, médica y bancaria.Años 2000 en adelante: Ágil, automatización y DevOps
Con la publicación del Manifiesto Ágil en 2001, la calidad se enfocó en entregas rápidas y constantes, colaboración con el cliente y adaptación al cambio. Las metodologías ágiles como Scrum, XP y Kanban incorporan pruebas continuas, integración frecuente y retrospectivas como parte de la cultura de mejora. Se dio paso a una visión más flexible y orientada al valor.
Más recientemente, prácticas como DevOps, Integración Continua (CI) y Entrega Continua (CD) han revolucionado la calidad del software al automatizar pruebas, despliegues y monitoreo. Esto ha permitido una mejora constante y retroalimentación inmediata en entornos productivos. La calidad se convirtió en una responsabilidad compartida por todos los equipos, no solo del área de pruebas.
Hoy en día, conceptos como pruebas automatizadas, monitorización en tiempo real, análisis predictivo y el uso de inteligencia artificial en control de calidad forman parte de un enfoque moderno que busca reducir errores, predecir fallos antes de que ocurran y mejorar la experiencia del usuario final.
